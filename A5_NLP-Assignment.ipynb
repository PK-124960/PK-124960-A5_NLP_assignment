{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“˜ Assignment 5: Optimization Human Preference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Task 1:** Finding a Suitable Dataset\n",
    "\n",
    "Goal: Select and preprocess a dataset to create prompt-response pairs for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… English Dataset Loaded: 39283 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# âœ… Load the dataset\n",
    "dataset = load_dataset(\"OpenAssistant/oasst1\")\n",
    "\n",
    "# âœ… Filter English messages\n",
    "english_data = dataset['train'].filter(lambda x: x['lang'] == 'en')\n",
    "print(f\"âœ… English Dataset Loaded: {len(english_data)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Extract prompt-response pairs\n",
    "pairs = []\n",
    "for conversation in english_data:\n",
    "    if isinstance(conversation, dict) and 'messages' in conversation:\n",
    "        messages = conversation['messages']\n",
    "        for i in range(len(messages) - 1):\n",
    "            if messages[i]['role'] == 'user' and messages[i + 1]['role'] == 'assistant':\n",
    "                pairs.append({\n",
    "                    'prompt': messages[i]['text'],\n",
    "                    'response': messages[i + 1]['text'],\n",
    "                    'quality': messages[i + 1].get('quality', 0)  # Default quality if missing\n",
    "                })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")  # Example: GPT-2 model\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Fix padding issue\n",
    "\n",
    "# âœ… Tokenize dataset\n",
    "tokenized_pairs = []\n",
    "for pair in pairs:\n",
    "    tokenized_input = tokenizer(pair['prompt'], truncation=True, padding='max_length', max_length=128)\n",
    "    tokenized_output = tokenizer(pair['response'], truncation=True, padding='max_length', max_length=128)\n",
    "    tokenized_pairs.append({\n",
    "        'input_ids': tokenized_input['input_ids'],\n",
    "        'attention_mask': tokenized_input['attention_mask'],\n",
    "        'labels': tokenized_output['input_ids'],\n",
    "        'quality': pair['quality']\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Create a PyTorch dataset\n",
    "class PreferenceDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            'input_ids': torch.tensor(item['input_ids']),\n",
    "            'attention_mask': torch.tensor(item['attention_mask']),\n",
    "            'labels': torch.tensor(item['labels']),\n",
    "            'quality': torch.tensor(item['quality'])\n",
    "        }\n",
    "\n",
    "# âœ… Initialize dataset\n",
    "train_dataset = PreferenceDataset(tokenized_pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Task 2:** Training a Model with DPOTrainer \n",
    "\n",
    "Goal: Train a reward model using a pre-trained transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# âœ… Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.empty_cache()  # Free CUDA memory\n",
    "print(f\"ðŸ”¹ Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset Loaded: 6312 samples\n"
     ]
    }
   ],
   "source": [
    "# âœ… Load dataset\n",
    "dataset = load_dataset(\"openai/summarize_from_feedback\", \"axis\", split=\"test\")  # Use test split\n",
    "print(f\"âœ… Dataset Loaded: {len(dataset)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# âœ… Load a pre-trained Transformer model (GPT-2 for reward modeling)\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# âœ… Fix the tokenizer padding issue (GPT models donâ€™t have a default pad token)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use EOS as padding token\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)\n",
    "reward_model.config.pad_token_id = tokenizer.pad_token_id  # Ensure model uses pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Preprocessing function\n",
    "def preprocess_data(example):\n",
    "    \"\"\"Processes dataset to extract chosen and rejected responses for reward training.\"\"\"\n",
    "    return {\n",
    "        \"prompt\": str(example.get(\"info\", {}).get(\"post\", \"\")),  # Convert to string\n",
    "        \"chosen\": str(example.get(\"summary\", \"\")),  # Preferred summary\n",
    "        \"rejected\": str(example.get(\"summary\", \"\"))  # Duplicate since dataset only has 'summary'\n",
    "    }\n",
    "\n",
    "# âœ… Apply preprocessing safely\n",
    "dataset = dataset.map(preprocess_data, remove_columns=[\"info\", \"summary\", \"worker\", \"batch\", \"split\"], features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen Outputs Shape: torch.Size([6312, 128])\n",
      "Rejected Outputs Shape: torch.Size([6312, 128])\n"
     ]
    }
   ],
   "source": [
    "# âœ… Convert dataset to list format\n",
    "prompts = [entry[\"prompt\"] for entry in dataset]\n",
    "chosen_responses = [entry[\"chosen\"] for entry in dataset]\n",
    "rejected_responses = [entry[\"rejected\"] for entry in dataset]\n",
    "\n",
    "# âœ… Tokenize inputs\n",
    "chosen_outputs = tokenizer(chosen_responses, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "rejected_outputs = tokenizer(rejected_responses, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# âœ… Ensure tensors are non-empty\n",
    "print(\"Chosen Outputs Shape:\", chosen_outputs[\"input_ids\"].shape)\n",
    "print(\"Rejected Outputs Shape:\", rejected_outputs[\"input_ids\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Define optimizer & loss function\n",
    "optimizer = optim.AdamW(reward_model.parameters(), lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# âœ… Ensure labels match the shape of chosen/rejected outputs\n",
    "num_samples = len(chosen_outputs[\"input_ids\"])\n",
    "labels = torch.cat([\n",
    "    torch.ones(num_samples, dtype=torch.float32),\n",
    "    torch.zeros(num_samples, dtype=torch.float32)\n",
    "]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0000\n",
      "Epoch 2, Loss: 0.0000\n",
      "Epoch 3, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4  \n",
    "\n",
    "# âœ… Training function\n",
    "def train_reward_model(model, optimizer, chosen_outputs, rejected_outputs, labels, epochs=3):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i in range(0, len(chosen_outputs[\"input_ids\"]), BATCH_SIZE):\n",
    "            # Select batch\n",
    "            batch_chosen = {k: v[i:i+BATCH_SIZE].to(device) for k, v in chosen_outputs.items()}\n",
    "            batch_rejected = {k: v[i:i+BATCH_SIZE].to(device) for k, v in rejected_outputs.items()}\n",
    "            batch_labels = labels[i:i+BATCH_SIZE]\n",
    "\n",
    "            # âœ… Ensure label shape matches model output\n",
    "            chosen_scores = model(**batch_chosen).logits.squeeze()\n",
    "            rejected_scores = model(**batch_rejected).logits.squeeze()\n",
    "\n",
    "            # âœ… Check for empty tensors before computing loss\n",
    "            if chosen_scores.shape[0] == 0 or rejected_scores.shape[0] == 0:\n",
    "                print(\"Skipping empty batch...\")\n",
    "                continue\n",
    "\n",
    "            # âœ… Fix label slicing to match logits shape\n",
    "            loss = criterion(chosen_scores, batch_labels[:chosen_scores.shape[0]]) + criterion(rejected_scores, batch_labels[:rejected_scores.shape[0]])\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Free unused memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# âœ… Train the reward model\n",
    "train_reward_model(reward_model, optimizer, chosen_outputs, rejected_outputs, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Reward model training complete & saved!\n"
     ]
    }
   ],
   "source": [
    "# âœ… Save trained reward model\n",
    "torch.save(reward_model.state_dict(), \"reward_model.pth\")\n",
    "print(\"âœ… Reward model training complete & saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# âœ… Set device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ”¹ Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# âœ… Load trained reward model from Task 2\n",
    "model_name = \"gpt2\"\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)\n",
    "reward_model.load_state_dict(torch.load(\"reward_model.pth\", map_location=device))\n",
    "reward_model.config.pad_token_id = reward_model.config.eos_token_id  # Ensure pad token is set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset Loaded: 6312 samples\n"
     ]
    }
   ],
   "source": [
    "# âœ… Load dataset (Using OpenAI Summarization Feedback dataset)\n",
    "dataset = load_dataset(\"openai/summarize_from_feedback\", \"axis\", split=\"test\")  # Use test split\n",
    "print(f\"âœ… Dataset Loaded: {len(dataset)} samples\")\n",
    "\n",
    "# âœ… Load tokenizer & set padding\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Use EOS as padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Preprocessing function for DPO\n",
    "def preprocess_data(example):\n",
    "    \"\"\"Processes dataset for DPO training.\"\"\"\n",
    "    return {\n",
    "        \"prompt\": str(example.get(\"info\", {}).get(\"post\", \"\")),  # Extract original post\n",
    "        \"chosen\": str(example.get(\"summary\", \"\")),  # Preferred summary\n",
    "        \"rejected\": str(example.get(\"summary\", \"\"))  # Currently duplicating since dataset only has 'summary'\n",
    "    }\n",
    "\n",
    "# âœ… Apply preprocessing\n",
    "dataset = dataset.map(preprocess_data, remove_columns=[\"info\", \"summary\", \"worker\", \"batch\", \"split\"], features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Convert dataset to list format\n",
    "prompts = [entry[\"prompt\"] for entry in dataset]\n",
    "chosen_responses = [entry[\"chosen\"] for entry in dataset]\n",
    "rejected_responses = [entry[\"rejected\"] for entry in dataset]\n",
    "\n",
    "# âœ… Tokenize inputs\n",
    "chosen_outputs = tokenizer(chosen_responses, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "rejected_outputs = tokenizer(rejected_responses, padding=True, truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "\n",
    "# âœ… Convert dataset to PyTorch tensors\n",
    "inputs = tokenizer(prompts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\").to(device)\n",
    "chosen_inputs = {k: v.to(device) for k, v in chosen_outputs.items()}\n",
    "rejected_inputs = {k: v.to(device) for k, v in rejected_outputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Define DPO loss function\n",
    "def dpo_loss(chosen_scores, rejected_scores, beta=0.1):\n",
    "    \"\"\"\n",
    "    Implements Direct Preference Optimization (DPO) loss.\n",
    "\n",
    "    Args:\n",
    "    - chosen_scores: Model outputs for chosen responses\n",
    "    - rejected_scores: Model outputs for rejected responses\n",
    "    - beta: Temperature parameter for softmax scaling\n",
    "\n",
    "    Returns:\n",
    "    - Loss value\n",
    "    \"\"\"\n",
    "    logits_diff = chosen_scores - rejected_scores\n",
    "    loss = -torch.log(torch.sigmoid(logits_diff / beta)).mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.6842\n",
      "Epoch 2, Loss: 0.7656\n",
      "Epoch 3, Loss: 0.8220\n",
      "Epoch 4, Loss: 0.7620\n",
      "Epoch 5, Loss: 0.6606\n",
      "Epoch 6, Loss: 0.8564\n",
      "Epoch 7, Loss: 0.8131\n",
      "Epoch 8, Loss: 0.7314\n",
      "Epoch 9, Loss: 0.8610\n",
      "Epoch 10, Loss: 0.6738\n"
     ]
    }
   ],
   "source": [
    "# âœ… Define optimizer\n",
    "optimizer = optim.AdamW(reward_model.parameters(), lr=1e-5)\n",
    "\n",
    "# âœ… Reduce batch size to prevent CUDA OOM\n",
    "BATCH_SIZE = 4  \n",
    "\n",
    "# âœ… Training function using DPO\n",
    "def train_dpo(model, optimizer, chosen_inputs, rejected_inputs, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for i in range(0, len(chosen_inputs[\"input_ids\"]), BATCH_SIZE):\n",
    "            # Select batch\n",
    "            batch_chosen = {k: v[i:i+BATCH_SIZE] for k, v in chosen_inputs.items()}\n",
    "            batch_rejected = {k: v[i:i+BATCH_SIZE] for k, v in rejected_inputs.items()}\n",
    "\n",
    "            # Compute logits\n",
    "            chosen_scores = model(**batch_chosen).logits.squeeze()\n",
    "            rejected_scores = model(**batch_rejected).logits.squeeze()\n",
    "\n",
    "            # Ensure correct tensor dimensions\n",
    "            if chosen_scores.shape[0] == 0 or rejected_scores.shape[0] == 0:\n",
    "                print(\"Skipping empty batch...\")\n",
    "                continue\n",
    "\n",
    "            # Compute DPO loss\n",
    "            loss = dpo_loss(chosen_scores, rejected_scores)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Free unused memory\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# âœ… Train the model with DPO\n",
    "train_dpo(reward_model, optimizer, chosen_inputs, rejected_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Task 3:** Pushing the Model to Hugging Face Hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DPO Fine-Tuning Complete & Model Saved!\n"
     ]
    }
   ],
   "source": [
    "# âœ… Save fine-tuned reward model\n",
    "torch.save(reward_model.state_dict(), \"reward_model_dpo.pth\")\n",
    "print(\"âœ… DPO Fine-Tuning Complete & Model Saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âœ… **Task 4:** Web Application Development"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
